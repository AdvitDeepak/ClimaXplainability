{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import climate_learn as cl\n",
    "\n",
    "# root_directory = \"/home/prateiksinha/new_data/mpi\"\n",
    "# variable = \"tas\"\n",
    "# cl.data.download_mpi_esm1_2_hr(\n",
    "#     dst=f\"{root_directory}/{variable}\",\n",
    "#     variable=variable,\n",
    "#     years=(1990, 2000), # optional, (1850, 2015) is the default range\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from climate_learn.data.processing.nc2npz import convert_nc2npz\n",
    "\n",
    "# convert_nc2npz(\n",
    "#     root_dir=\"/home/prateiksinha/new_data/awi\",\n",
    "#     save_dir=\"/home/prateiksinha/new_data/processed/mpi/\",\n",
    "#     variables=[\"temperature\"],\n",
    "#     start_train_year=1990,\n",
    "#     start_val_year=1998,\n",
    "#     start_test_year=1999,\n",
    "#     end_year=2000,\n",
    "#     num_shards=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HOURS_PER_YEAR = 8736  # 8760 --> 8736 which is dividable by 16\n",
    "\n",
    "NAME_TO_VAR = {\n",
    "    \"2m_temperature\": \"tas\",\n",
    "    # \"2m_temperature\": \"t2m\",\n",
    "    \"10m_u_component_of_wind\": \"u10\",\n",
    "    \"10m_v_component_of_wind\": \"v10\",\n",
    "    \"mean_sea_level_pressure\": \"msl\",\n",
    "    \"surface_pressure\": \"sp\",\n",
    "    \"toa_incident_solar_radiation\": \"tisr\",\n",
    "    \"total_precipitation\": \"tp\",\n",
    "    \"land_sea_mask\": \"lsm\",\n",
    "    \"orography\": \"orography\",\n",
    "    \"lattitude\": \"lat2d\",\n",
    "    \"geopotential\": \"z\",\n",
    "    \"u_component_of_wind\": \"u\",\n",
    "    \"v_component_of_wind\": \"v\",\n",
    "    \"temperature\": \"t\",\n",
    "    \"relative_humidity\": \"r\",\n",
    "    \"specific_humidity\": \"q\",\n",
    "    \"vorticity\": \"vo\",\n",
    "    \"potential_vorticity\": \"pv\",\n",
    "    \"total_cloud_cover\": \"tcc\",\n",
    "}\n",
    "\n",
    "VAR_TO_NAME = {v: k for k, v in NAME_TO_VAR.items()}\n",
    "\n",
    "\n",
    "def nc2np(path, variables, years, save_dir, partition, num_shards_per_year):\n",
    "    os.makedirs(os.path.join(save_dir, partition), exist_ok=True)\n",
    "\n",
    "    if partition == \"train\":\n",
    "        normalize_mean = {}\n",
    "        normalize_std = {}\n",
    "    climatology = {}\n",
    "\n",
    "    constants_path = os.path.join(path, \"constants.nc\")\n",
    "    constants_are_downloaded = os.path.isfile(constants_path)\n",
    "\n",
    "    if constants_are_downloaded:\n",
    "        print('FOUND CONSTANTS')\n",
    "        constants = xr.open_mfdataset(\n",
    "            constants_path, combine=\"by_coords\", parallel=True\n",
    "        )\n",
    "        constant_fields = [VAR_TO_NAME[v] for v in CONSTANTS if v in VAR_TO_NAME.keys()]\n",
    "        constant_values = {}\n",
    "        for f in constant_fields:\n",
    "            constant_values[f] = np.expand_dims(\n",
    "                constants[NAME_TO_VAR[f]].to_numpy(), axis=(0, 1)\n",
    "            ).repeat(HOURS_PER_YEAR, axis=0)\n",
    "            if partition == \"train\":\n",
    "                normalize_mean[f] = constant_values[f].mean(axis=(0, 2, 3))\n",
    "                normalize_std[f] = constant_values[f].std(axis=(0, 2, 3))\n",
    "\n",
    "    for year in tqdm(years):\n",
    "        np_vars = {}\n",
    "\n",
    "        # constant variables\n",
    "        if constants_are_downloaded:\n",
    "            for f in constant_fields:\n",
    "                np_vars[f] = constant_values[f]\n",
    "\n",
    "        # non-constant fields\n",
    "        for var in variables:\n",
    "            ps = glob.glob(os.path.join(path, var, f\"*{year}*.nc\"))\n",
    "            ds = xr.open_mfdataset(\n",
    "                ps, combine=\"by_coords\", parallel=True\n",
    "            )  # dataset for a single variable\n",
    "            code = NAME_TO_VAR[var]\n",
    "\n",
    "            if len(ds[code].shape) == 3:  # surface level variables\n",
    "                ds[code] = ds[code].expand_dims(\"val\", axis=1)\n",
    "                # remove the last 24 hours if this year has 366 days\n",
    "                if code == \"tp\":  # accumulate 6 hours and log transform\n",
    "                    tp = ds[code].to_numpy()\n",
    "                    tp_cum_6hrs = np.cumsum(tp, axis=0)\n",
    "                    tp_cum_6hrs[6:] = tp_cum_6hrs[6:] - tp_cum_6hrs[:-6]\n",
    "                    eps = 0.001\n",
    "                    tp_cum_6hrs = np.log(eps + tp_cum_6hrs) - np.log(eps)\n",
    "                    np_vars[var] = tp_cum_6hrs[-HOURS_PER_YEAR:]\n",
    "                else:\n",
    "                    np_vars[var] = ds[code].to_numpy()[-HOURS_PER_YEAR:]\n",
    "\n",
    "                if partition == \"train\":\n",
    "                    # compute mean and std of each var in each year\n",
    "                    var_mean_yearly = np_vars[var].mean(axis=(0, 2, 3))\n",
    "                    var_std_yearly = np_vars[var].std(axis=(0, 2, 3))\n",
    "                    if var not in normalize_mean:\n",
    "                        normalize_mean[var] = [var_mean_yearly]\n",
    "                        normalize_std[var] = [var_std_yearly]\n",
    "                    else:\n",
    "                        normalize_mean[var].append(var_mean_yearly)\n",
    "                        normalize_std[var].append(var_std_yearly)\n",
    "\n",
    "                clim_yearly = np_vars[var].mean(axis=0)\n",
    "                if var not in climatology:\n",
    "                    climatology[var] = [clim_yearly]\n",
    "                else:\n",
    "                    climatology[var].append(clim_yearly)\n",
    "\n",
    "            else:  # pressure-level variables\n",
    "                assert len(ds[code].shape) == 4\n",
    "                all_levels = ds[\"level\"][:].to_numpy()\n",
    "                all_levels = np.intersect1d(all_levels, DEFAULT_PRESSURE_LEVELS)\n",
    "                for level in all_levels:\n",
    "                    ds_level = ds.sel(level=[level])\n",
    "                    level = int(level)\n",
    "                    # remove the last 24 hours if this year has 366 days\n",
    "                    np_vars[f\"{var}_{level}\"] = ds_level[code].to_numpy()[\n",
    "                        -HOURS_PER_YEAR:\n",
    "                    ]\n",
    "\n",
    "                    if partition == \"train\":\n",
    "                        # compute mean and std of each var in each year\n",
    "                        var_mean_yearly = np_vars[f\"{var}_{level}\"].mean(axis=(0, 2, 3))\n",
    "                        var_std_yearly = np_vars[f\"{var}_{level}\"].std(axis=(0, 2, 3))\n",
    "                        if f\"{var}_{level}\" not in normalize_mean:\n",
    "                            normalize_mean[f\"{var}_{level}\"] = [var_mean_yearly]\n",
    "                            normalize_std[f\"{var}_{level}\"] = [var_std_yearly]\n",
    "                        else:\n",
    "                            normalize_mean[f\"{var}_{level}\"].append(var_mean_yearly)\n",
    "                            normalize_std[f\"{var}_{level}\"].append(var_std_yearly)\n",
    "\n",
    "                    clim_yearly = np_vars[f\"{var}_{level}\"].mean(axis=0)\n",
    "                    if f\"{var}_{level}\" not in climatology:\n",
    "                        climatology[f\"{var}_{level}\"] = [clim_yearly]\n",
    "                    else:\n",
    "                        climatology[f\"{var}_{level}\"].append(clim_yearly)\n",
    "\n",
    "        assert HOURS_PER_YEAR % num_shards_per_year == 0\n",
    "        num_hrs_per_shard = HOURS_PER_YEAR // num_shards_per_year\n",
    "        for shard_id in range(num_shards_per_year):\n",
    "            start_id = shard_id * num_hrs_per_shard\n",
    "            end_id = start_id + num_hrs_per_shard\n",
    "            sharded_data = {k: np_vars[k][start_id:end_id] for k in np_vars.keys()}\n",
    "            np.savez(\n",
    "                os.path.join(save_dir, partition, f\"{year}_{shard_id}.npz\"),\n",
    "                **sharded_data,\n",
    "            )\n",
    "\n",
    "    if partition == \"train\":\n",
    "        for var in normalize_mean.keys():\n",
    "            if not constants_are_downloaded or var not in constant_fields:\n",
    "                normalize_mean[var] = np.stack(normalize_mean[var], axis=0)\n",
    "                normalize_std[var] = np.stack(normalize_std[var], axis=0)\n",
    "\n",
    "        for var in normalize_mean.keys():  # aggregate over the years\n",
    "            if not constants_are_downloaded or var not in constant_fields:\n",
    "                mean, std = normalize_mean[var], normalize_std[var]\n",
    "                # var(X) = E[var(X|Y)] + var(E[X|Y])\n",
    "                variance = (\n",
    "                    (std**2).mean(axis=0)\n",
    "                    + (mean**2).mean(axis=0)\n",
    "                    - mean.mean(axis=0) ** 2\n",
    "                )\n",
    "                std = np.sqrt(variance)\n",
    "                # E[X] = E[E[X|Y]]\n",
    "                mean = mean.mean(axis=0)\n",
    "                normalize_mean[var] = mean\n",
    "                if var == \"total_precipitation\":\n",
    "                    normalize_mean[var] = np.zeros_like(normalize_mean[var])\n",
    "                normalize_std[var] = std\n",
    "\n",
    "        np.savez(os.path.join(save_dir, \"normalize_mean.npz\"), **normalize_mean)\n",
    "        np.savez(os.path.join(save_dir, \"normalize_std.npz\"), **normalize_std)\n",
    "\n",
    "    for var in climatology.keys():\n",
    "        climatology[var] = np.stack(climatology[var], axis=0)\n",
    "    climatology = {k: np.mean(v, axis=0) for k, v in climatology.items()}\n",
    "    np.savez(\n",
    "        os.path.join(save_dir, partition, \"climatology.npz\"),\n",
    "        **climatology,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 36.62it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "import glob \n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "root_dir = '/home/prateiksinha/new_data/mpi_regridded'\n",
    "variables = ['2m_temperature']\n",
    "train_years = range(1990, 2000)\n",
    "save_dir = \"/home/prateiksinha/new_data/processed/mpi\"\n",
    "\n",
    "nc2np(root_dir, variables, train_years, save_dir, 'test', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lat and lon data\n",
    "# root_dir = '/home/prateiksinha/new_data/awi'\n",
    "# variables = ['2m_temperature']\n",
    "# train_years = range(1990, 2000)\n",
    "# save_dir = \"/home/prateiksinha/new_data/processed/awi\"\n",
    "\n",
    "ps = glob.glob(os.path.join(root_dir, variables[0], f\"*{train_years[0]}*.nc\"))\n",
    "x = xr.open_mfdataset(ps[0], parallel=True)\n",
    "lat = np.array(x[\"lat\"])\n",
    "lon = np.array(x[\"lon\"])\n",
    "np.save(os.path.join(save_dir, \"lat.npy\"), lat)\n",
    "np.save(os.path.join(save_dir, \"lon.npy\"), lon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climaX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
